{"dependencies": [{"name": "altair", "version": "5.5.0", "vulns": []}, {"name": "attrs", "version": "25.1.0", "vulns": []}, {"name": "blinker", "version": "1.9.0", "vulns": []}, {"name": "cachetools", "version": "5.5.1", "vulns": []}, {"name": "certifi", "version": "2025.1.31", "vulns": []}, {"name": "charset-normalizer", "version": "3.4.1", "vulns": []}, {"name": "click", "version": "8.1.8", "vulns": []}, {"name": "gitdb", "version": "4.0.12", "vulns": []}, {"name": "smmap", "version": "5.0.2", "vulns": []}, {"name": "gitpython", "version": "3.1.44", "vulns": []}, {"name": "greenlet", "version": "3.1.1", "vulns": []}, {"name": "idna", "version": "3.10", "vulns": []}, {"name": "jinja2", "version": "3.1.5", "vulns": [{"id": "CVE-2025-27516", "fix_versions": ["3.1.6"], "aliases": ["GHSA-cpwx-vrp4-4pq7"], "description": "An oversight in how the Jinja sandboxed environment interacts with the `|attr` filter allows an attacker that controls the content of a template to execute arbitrary Python code.  To exploit the vulnerability, an attacker needs to control the content of a template. Whether that is the case depends on the type of application using Jinja. This vulnerability impacts users of applications which execute untrusted templates.  Jinja's sandbox does catch calls to `str.format` and ensures they don't escape the sandbox. However, it's possible to use the `|attr` filter to get a reference to a string's plain format method, bypassing the sandbox. After the fix, the `|attr` filter no longer bypasses the environment's attribute lookup."}]}, {"name": "jsonschema", "version": "4.23.0", "vulns": []}, {"name": "jsonschema-specifications", "version": "2024.10.1", "vulns": []}, {"name": "markdown-it-py", "version": "3.0.0", "vulns": []}, {"name": "mdurl", "version": "0.1.2", "vulns": []}, {"name": "markupsafe", "version": "3.0.2", "vulns": []}, {"name": "narwhals", "version": "1.26.0", "vulns": []}, {"name": "numpy", "version": "2.2.2", "vulns": []}, {"name": "packaging", "version": "24.2", "vulns": []}, {"name": "pandas", "version": "2.2.3", "vulns": []}, {"name": "patsy", "version": "1.0.2", "vulns": []}, {"name": "pillow", "version": "11.1.0", "vulns": [{"id": "CVE-2026-25990", "fix_versions": ["12.1.1"], "aliases": ["GHSA-cfh3-3jmp-rvhc"], "description": "### Impact An out-of-bounds write may be triggered when loading a specially crafted PSD image. Pillow >= 10.3.0 users are affected.  ### Patches Pillow 12.1.1 will be released shortly with a fix for this.  ### Workarounds `Image.open()` has a `formats` parameter that can be used to prevent PSD images from being opened.  ### References Pillow 12.1.1 will add release notes at https://pillow.readthedocs.io/en/stable/releasenotes/index.html"}]}, {"name": "plotly", "version": "6.0.0", "vulns": []}, {"name": "plotly-express", "version": "0.4.1", "vulns": []}, {"name": "protobuf", "version": "5.29.3", "vulns": [{"id": "CVE-2025-4565", "fix_versions": ["4.25.8", "5.29.5", "6.31.1"], "aliases": ["GHSA-8qvm-5x2c-j2w7"], "description": "### Summary Any project that uses Protobuf pure-Python backend to parse untrusted Protocol Buffers data containing an arbitrary number of **recursive groups**, **recursive messages** or **a series of [`SGROUP`](https://protobuf.dev/programming-guides/encoding/#groups) tags** can be corrupted by exceeding the Python recursion limit.  Reporter: Alexis Challande, Trail of Bits Ecosystem Security Team [ecosystem@trailofbits.com](mailto:ecosystem@trailofbits.com)  Affected versions: This issue only affects the [pure-Python implementation](https://github.com/protocolbuffers/protobuf/tree/main/python#implementation-backends) of protobuf-python backend. This is the implementation when `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` environment variable is set or the default when protobuf is used from Bazel or pure-Python PyPi wheels. CPython PyPi wheels do not use pure-Python by default.  This is a Python variant of a [previous issue affecting protobuf-java](https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-735f-pc8j-v9w8).  ### Severity This is a potential Denial of Service. Parsing nested protobuf data creates unbounded recursions that can be abused by an attacker.  ### Proof of Concept For reproduction details, please refer to the unit tests [decoder_test.py](https://github.com/protocolbuffers/protobuf/blob/main/python/google/protobuf/internal/decoder_test.py#L87-L98) and [message_test](https://github.com/protocolbuffers/protobuf/blob/main/python/google/protobuf/internal/message_test.py#L1436-L1478)  ### Remediation and Mitigation A mitigation is available now. Please update to the latest available versions of the following packages: * protobuf-python(4.25.8, 5.29.5, 6.31.1)"}, {"id": "CVE-2026-0994", "fix_versions": ["5.29.6", "6.33.5"], "aliases": ["GHSA-7gcm-g887-7qv7"], "description": "A denial-of-service (DoS) vulnerability exists in google.protobuf.json_format.ParseDict() in Python, where the max_recursion_depth limit can be bypassed when parsing nested google.protobuf.Any messages.  Due to missing recursion depth accounting inside the internal Any-handling logic, an attacker can supply deeply nested Any structures that bypass the intended recursion limit, eventually exhausting Python\u2019s recursion stack and causing a RecursionError."}]}, {"name": "psycopg2-binary", "version": "2.9.11", "vulns": []}, {"name": "pyarrow", "version": "19.0.0", "vulns": []}, {"name": "pydeck", "version": "0.9.1", "vulns": []}, {"name": "pygments", "version": "2.19.1", "vulns": []}, {"name": "python-dateutil", "version": "2.9.0.post0", "vulns": []}, {"name": "pytz", "version": "2025.1", "vulns": []}, {"name": "referencing", "version": "0.36.2", "vulns": []}, {"name": "requests", "version": "2.32.3", "vulns": [{"id": "CVE-2024-47081", "fix_versions": ["2.32.4"], "aliases": ["GHSA-9hjg-9r4m-mvj7"], "description": "### Impact  Due to a URL parsing issue, Requests releases prior to 2.32.4 may leak .netrc credentials to third parties for specific maliciously-crafted URLs.  ### Workarounds For older versions of Requests, use of the .netrc file can be disabled with `trust_env=False` on your Requests Session ([docs](https://requests.readthedocs.io/en/latest/api/#requests.Session.trust_env)).  ### References https://github.com/psf/requests/pull/6965 https://seclists.org/fulldisclosure/2025/Jun/2"}]}, {"name": "urllib3", "version": "2.3.0", "vulns": [{"id": "CVE-2025-50182", "fix_versions": ["2.5.0"], "aliases": ["GHSA-48p4-8xcf-vxj5"], "description": "urllib3 [supports](https://urllib3.readthedocs.io/en/2.4.0/reference/contrib/emscripten.html) being used in a Pyodide runtime utilizing the [JavaScript Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) or falling back on [XMLHttpRequest](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest). This means you can use Python libraries to make HTTP requests from your browser or Node.js. Additionally, urllib3 provides [a mechanism](https://urllib3.readthedocs.io/en/2.4.0/user-guide.html#retrying-requests) to control redirects.  However, the `retries` and `redirect` parameters are ignored with Pyodide; the runtime itself determines redirect behavior.   ## Affected usages  Any code which relies on urllib3 to control the number of redirects for an HTTP request in a Pyodide runtime.   ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects may remain vulnerable if a Pyodide runtime redirect mechanism is unsuitable.   ## Remediation  If you use urllib3 in Node.js, upgrade to a patched version of urllib3.  Unfortunately, browsers provide no suitable way which urllib3 can use: `XMLHttpRequest` provides no control over redirects, the Fetch API returns `opaqueredirect` responses lacking data when redirects are controlled manually. Expect default browser behavior for redirects."}, {"id": "CVE-2025-50181", "fix_versions": ["2.5.0"], "aliases": ["GHSA-pq67-6m6q-mj2v"], "description": "urllib3 handles redirects and retries using the same mechanism, which is controlled by the `Retry` object. The most common way to disable redirects is at the request level, as follows:  ```python resp = urllib3.request(\"GET\", \"https://httpbin.org/redirect/1\", redirect=False) print(resp.status) # 302 ```  However, it is also possible to disable redirects, for all requests, by instantiating a `PoolManager` and specifying `retries` in a way that disable redirects:  ```python import urllib3  http = urllib3.PoolManager(retries=0)  # should raise MaxRetryError on redirect http = urllib3.PoolManager(retries=urllib3.Retry(redirect=0))  # equivalent to the above http = urllib3.PoolManager(retries=False)  # should return the first response  resp = http.request(\"GET\", \"https://httpbin.org/redirect/1\") ```  However, the `retries` parameter is currently ignored, which means all the above examples don't disable redirects.  ## Affected usages  Passing `retries` on `PoolManager` instantiation to disable redirects or restrict their number.  By default, requests and botocore users are not affected.  ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects at the PoolManager level will remain vulnerable.  ## Remediation  You can remediate this vulnerability with the following steps:   * Upgrade to a patched version of urllib3. If your organization would benefit from the continued support of urllib3 1.x, please contact [sethmichaellarson@gmail.com](mailto:sethmichaellarson@gmail.com) to discuss sponsorship or contribution opportunities.  * Disable redirects at the `request()` level instead of the `PoolManager()` level."}, {"id": "CVE-2025-66418", "fix_versions": ["2.6.0"], "aliases": ["GHSA-gm62-xv2j-4w53"], "description": "## Impact  urllib3 supports chained HTTP encoding algorithms for response content according to RFC 9110 (e.g., `Content-Encoding: gzip, zstd`).  However, the number of links in the decompression chain was unbounded allowing a malicious server to insert a virtually unlimited number of compression steps leading to high CPU usage and massive memory allocation for the decompressed data.   ## Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier for HTTP requests to untrusted sources unless they disable content decoding explicitly.   ## Remediation  Upgrade to at least urllib3 v2.6.0 in which the library limits the number of links to 5.  If upgrading is not immediately possible, use [`preload_content=False`](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) and ensure that `resp.headers[\"content-encoding\"]` contains a safe number of encodings before reading the response content."}, {"id": "CVE-2025-66471", "fix_versions": ["2.6.0"], "aliases": ["GHSA-2xpw-w6gg-jr37"], "description": "### Impact  urllib3's [streaming API](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) is designed for the efficient handling of large HTTP responses by reading the content in chunks, rather than loading the entire response body into memory at once.  When streaming a compressed response, urllib3 can perform decoding or decompression based on the HTTP `Content-Encoding` header (e.g., `gzip`, `deflate`, `br`, or `zstd`). The library must read compressed data from the network and decompress it until the requested chunk size is met. Any resulting decompressed data that exceeds the requested amount is held in an internal buffer for the next read operation.  The decompression logic could cause urllib3 to fully decode a small amount of highly compressed data in a single operation. This can result in excessive resource consumption (high CPU usage and massive memory allocation for the decompressed data; CWE-409) on the client side, even if the application only requested a small chunk of data.   ### Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier to stream large compressed responses or content from untrusted sources.  `stream()`, `read(amt=256)`, `read1(amt=256)`, `read_chunked(amt=256)`, `readinto(b)` are examples of `urllib3.HTTPResponse` method calls using the affected logic unless decoding is disabled explicitly.   ### Remediation  Upgrade to at least urllib3 v2.6.0 in which the library avoids decompressing data that exceeds the requested amount.  If your environment contains a package facilitating the Brotli encoding, upgrade to at least Brotli 1.2.0 or brotlicffi 1.2.0.0 too. These versions are enforced by the `urllib3[brotli]` extra in the patched versions of urllib3.   ### Credits  The issue was reported by @Cycloctane. Supplemental information was provided by @stamparm during a security audit performed by [7ASecurity](https://7asecurity.com/) and facilitated by [OSTIF](https://ostif.org/)."}, {"id": "CVE-2026-21441", "fix_versions": ["2.6.3"], "aliases": ["GHSA-38jv-5279-wg99"], "description": "### Impact  urllib3's [streaming API](https://urllib3.readthedocs.io/en/2.6.2/advanced-usage.html#streaming-and-i-o) is designed for the efficient handling of large HTTP responses by reading the content in chunks, rather than loading the entire response body into memory at once.  urllib3 can perform decoding or decompression based on the HTTP `Content-Encoding` header (e.g., `gzip`, `deflate`, `br`, or `zstd`). When using the streaming API, the library decompresses only the necessary bytes, enabling partial content consumption.  However, for HTTP redirect responses, the library would read the entire response body to drain the connection and decompress the content unnecessarily. This decompression occurred even before any read methods were called, and configured read limits did not restrict the amount of decompressed data. As a result, there was no safeguard against decompression bombs. A malicious server could exploit this to trigger excessive resource consumption on the client (high CPU usage and large memory allocations for decompressed data; CWE-409).  ### Affected usages  Applications and libraries using urllib3 version 2.6.2 and earlier to stream content from untrusted sources by setting `preload_content=False` when they do not disable redirects.   ### Remediation  Upgrade to at least urllib3 v2.6.3 in which the library does not decode content of redirect responses when `preload_content=False`.  If upgrading is not immediately possible, disable [redirects](https://urllib3.readthedocs.io/en/2.6.2/user-guide.html#retrying-requests) by setting `redirect=False` for requests to untrusted source."}]}, {"name": "rich", "version": "13.9.4", "vulns": []}, {"name": "rpds-py", "version": "0.22.3", "vulns": []}, {"name": "scipy", "version": "1.17.0", "vulns": []}, {"name": "six", "version": "1.17.0", "vulns": []}, {"name": "sqlalchemy", "version": "2.0.38", "vulns": []}, {"name": "statsmodels", "version": "0.14.6", "vulns": []}, {"name": "streamlit", "version": "1.42.0", "vulns": []}, {"name": "tenacity", "version": "9.0.0", "vulns": []}, {"name": "toml", "version": "0.10.2", "vulns": []}, {"name": "tornado", "version": "6.4.2", "vulns": [{"id": "CVE-2025-47287", "fix_versions": ["6.5"], "aliases": ["GHSA-7cx3-6m66-7c5m"], "description": "### Summary  When Tornado's ``multipart/form-data`` parser encounters certain errors, it logs a warning but continues trying to parse the remainder of the data. This allows remote attackers to generate an extremely high volume of logs, constituting a DoS attack. This DoS is compounded by the fact that the logging subsystem is synchronous.  ### Affected versions  All versions of Tornado prior to 6.5 are affected. The vulnerable parser is enabled by default.  ### Solution  Upgrade to Tornado version 6.5. In the meantime, risk can be mitigated by blocking `Content-Type: multipart/form-data` in a proxy."}]}, {"name": "typing-extensions", "version": "4.12.2", "vulns": []}, {"name": "watchdog", "version": "6.0.0", "vulns": []}, {"name": "tzdata", "version": "2025.1", "vulns": []}, {"name": "werkzeug", "version": "3.1.5", "vulns": []}], "fixes": []}
